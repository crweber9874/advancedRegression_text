<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction | A Course on Advanced Regression Models, Causal Inference, and Data Visualization</title>
  <meta name="description" content="These notes accompany POL 683" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction | A Course on Advanced Regression Models, Causal Inference, and Data Visualization" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These notes accompany POL 683" />
  <meta name="github-repo" content="crweber9874/advancedRegression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction | A Course on Advanced Regression Models, Causal Inference, and Data Visualization" />
  
  <meta name="twitter:description" content="These notes accompany POL 683" />
  

<meta name="author" content="Christopher Weber" />


<meta name="date" content="2024-08-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="syllabus.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">POL 683</a></li>

<li class="divider"></li>
<li><a href="index.html#section" id="toc-section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>1</b> Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#pol-683-advanced-regression-analysis"><i class="fa fa-check"></i>POL 683: Advanced Regression Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#fall-2024"><i class="fa fa-check"></i>Fall 2024</a></li>
</ul></li>
<li class="chapter" data-level="1.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>1.1</b> Course Description</a></li>
<li class="chapter" data-level="1.2" data-path="syllabus.html"><a href="syllabus.html#learning-objectives"><i class="fa fa-check"></i><b>1.2</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.3" data-path="syllabus.html"><a href="syllabus.html#grades"><i class="fa fa-check"></i><b>1.3</b> Grades</a></li>
<li class="chapter" data-level="1.4" data-path="syllabus.html"><a href="syllabus.html#course-outline"><i class="fa fa-check"></i><b>1.4</b> Course Outline</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="syllabus.html"><a href="syllabus.html#building-a-statistical-model"><i class="fa fa-check"></i><b>1.4.1</b> Building a Statistical Model</a></li>
<li class="chapter" data-level="1.4.2" data-path="syllabus.html"><a href="syllabus.html#estimation-and-model-fit"><i class="fa fa-check"></i><b>1.4.2</b> Estimation and Model Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#the-logic-of-inference"><i class="fa fa-check"></i><b>2.1</b> The Logic of Inference</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#conditional-probabilities-continued-bayes-rule"><i class="fa fa-check"></i><b>2.2</b> Conditional Probabilities, Continued: Bayes’ Rule</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#two-traditions"><i class="fa fa-check"></i><b>2.3</b> Two Traditions</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction.html"><a href="introduction.html#an-example-bernoulli-trials"><i class="fa fa-check"></i><b>2.3.1</b> An Example: Bernoulli Trials</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#simulation-and-the-likelihood-function"><i class="fa fa-check"></i><b>2.4</b> Simulation and the Likelihood Function</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#bayesian-analysis"><i class="fa fa-check"></i><b>2.4.1</b> Bayesian Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#quick-review"><i class="fa fa-check"></i><b>2.5</b> Quick Review</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#properties-of-estimators"><i class="fa fa-check"></i><b>2.6</b> Properties of Estimators</a></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#finite-sample-properties"><i class="fa fa-check"></i><b>2.7</b> Finite Sample Properties</a></li>
<li class="chapter" data-level="2.8" data-path="introduction.html"><a href="introduction.html#asymptotic-properties"><i class="fa fa-check"></i><b>2.8</b> Asymptotic Properties</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="introduction.html"><a href="introduction.html#the-bias-variance-tradeoff"><i class="fa fa-check"></i><b>2.8.1</b> The Bias Variance Tradeoff</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Course on Advanced Regression Models, Causal Inference, and Data Visualization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction<a href="introduction.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="the-logic-of-inference" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> The Logic of Inference<a href="introduction.html#the-logic-of-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This class involves much more than finding a model that is appropriate for a given data structure. In fact, we will continually be returning to two themes in inferential statistics, the <em>frequentist</em> and *Bayesian traditions. In some circumstances, these two traditions will yield very similar results, though the interpretations vary. During these first two weeks, we’ll consider these two perspectives from a theoretical vantage; throughout the rest of the term, we’ll review techniques to use these perspectives in inference.</p>
<p>It’s worthwhile to understand the motivating principles of these two approaches, prior to digging into technical content. Most likely, many of you have received training in the <em>frequentist</em> tradition of probability, which is often credited to Jerzy Neyman, Karl Pearson and Sir Ronald Fisher. The logic here is quite simple: Probability refers to the frequency of an event occurring in a series of trials.</p>
<p>Frequentism is actually quite intuitive when applied to circumstances in which we can observe repeated trials. If you flip a coin, the probability of observing heads is 0.5. What does that mean? Flip the coin 10000 times and you should expect to observe heads 5000 times.</p>
<p>Or consider the classic example of rolling a die. The probability of rolling a 1 (or any number, with a fair die) is 1/6. This number is of limited value in predicting the outcome of a single roll. Instead, what it means is that in a series of trials, approximately 1/6 of those trials will yield a 1. We could ask a variant of a classic game of chance question, which is if we have 10 trials, and each trial involves rolling two dice,is it beneficial to bet that at least one of those trials will yield two 1’s (see <a href="https://en.wikipedia.org/wiki/Problem_of_points" class="uri">https://en.wikipedia.org/wiki/Problem_of_points</a>)? The probability of observing two ones is 1/6 <span class="math inline">\(\times\)</span> 1/6 <span class="math inline">\(=\)</span> 1/36. This implies that the rest of the sample space (i.e., not rolling two ones) is 35/36. Since the rolls are independent, then the probability that two ones are not observed in 10 trials is <span class="math inline">\((35/36)^{10}=0.75\)</span> – thus one should probably bet on “snake eyes.”</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="introduction.html#cb1-1" tabindex="-1"></a><span class="co"># Load necessary library</span></span>
<span id="cb1-2"><a href="introduction.html#cb1-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-3"><a href="introduction.html#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="introduction.html#cb1-4" tabindex="-1"></a><span class="co"># Always set seed for reproducibility</span></span>
<span id="cb1-5"><a href="introduction.html#cb1-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) </span>
<span id="cb1-6"><a href="introduction.html#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="introduction.html#cb1-7" tabindex="-1"></a><span class="co"># Simulate draws</span></span>
<span id="cb1-8"><a href="introduction.html#cb1-8" tabindex="-1"></a>n_draws <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb1-9"><a href="introduction.html#cb1-9" tabindex="-1"></a>outcomes <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>), <span class="at">size =</span> n_draws, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb1-10"><a href="introduction.html#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="introduction.html#cb1-11" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb1-12"><a href="introduction.html#cb1-12" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Outcome =</span> outcomes)</span>
<span id="cb1-13"><a href="introduction.html#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="introduction.html#cb1-14" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb1-15"><a href="introduction.html#cb1-15" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> Outcome)) <span class="sc">+</span></span>
<span id="cb1-16"><a href="introduction.html#cb1-16" tabindex="-1"></a>  <span class="fu">geom_bar</span>() <span class="sc">+</span></span>
<span id="cb1-17"><a href="introduction.html#cb1-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Simulated Coin Flips&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Outcome&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="sc">+</span> </span>
<span id="cb1-18"><a href="introduction.html#cb1-18" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-1.png" width="480" /></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="introduction.html#cb2-1" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;Of &quot;</span>, n_draws, <span class="st">&quot; draws &quot;</span>,  <span class="fu">table</span>(df)[<span class="dv">1</span>], <span class="st">&quot; were heads! &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Of  10000  draws  4943  were heads! &quot;</code></pre>
<p>Be sure to set <em>set.seed(10000)</em> to ensure reproducibility.</p>
<p>Statistics is often taught from this perspective – even if not always clearly acknowledge. For instance, a confidence interval is not a statement of certainty regarding whether the derived interval contains the true population value. You were likely instructed, early on, that statements such as “there’s a 95% chance the mean falls between <lower bound> and <upper bound>’’ is incorrect. The reason is that frequentism hinges on the assumption that the population mean is fixed. If we were to repeatedly draw samples from a population and calculate a confidence interval, approximately 95% of those intervals would capture the population value value.</p>
<p>In fact, the very logic of inference hinges on the notion of drawing a subset of the population (a sample) and making an inference about the general population from that sample. Yet, the <strong>frequentist</strong> approach forces us to think of the population parameter as fixed – we simply don’t have it because it would be expensive to collect – and the methods we use to make inferences nearly always hinge on this notion of taking repeated samples from a population. Just recall the <strong>central limit theore</strong>, <strong>the standard error</strong>, <strong>confidence intervals</strong>, <strong>p-values</strong>, and <strong>Type I</strong> and <strong>Type II</strong> errors. These concepts all rely on the notion of repeated observation; hence, the frequentist’’ label. Let’s take a step back and consider what this means from the perspective of social science and sampling. Then, I think you’ll see what I mean when I use the term <strong>frequentist</strong>.</p>
<p><strong>A Step Back</strong></p>
<p>In this class, and really in most everything you’ve worked on, the focus of the research is rarely on individual observations but rather distributions. We can summarize data, and relationships between variables, based on distributions. For example, recall that in linear regression, one of the assumptions is the error process follows a parametric distribution (often the normal distribution). Moreoever, if we have a dependent variable that is clearly not normal and continuous, then this assumption becomes tenuous.</p>
<p>Recall the differences between a probability density function (PDF) and a continuous density function (CDF). A PDF gives the probability of an occurence (for a discrete variable) or a range of occurences (for a continous variable).</p>
<p><strong>Notation</strong></p>
<ol style="list-style-type: decimal">
<li><p>A CDF is written as F(x), or capital greek notation, e.g., <span class="math inline">\(\Gamma(x)\)</span>.</p></li>
<li><p>A PDF is written in lower case font, f(x), and we can find any area under a PDF by summation (for categorical data), and integration (for continuous data). That is,</p></li>
</ol>
<p><span class="math display">\[p(a&lt;x&lt;b)=\sum_i^{K} x_i\]</span></p>
<p><span class="math display">\[p(a&lt;x&lt;b)=\int f(x) dx\]</span></p>
<p>As such, <span class="math inline">\(F(\infty)=1\)</span>, and <span class="math inline">\(F(-\infty)=0\)</span>, and <span class="math inline">\(p(-\infty &lt; x &lt; \infty)=\int_{-\infty}^{\infty}f(x)dx=1\)</span>. This is an important principle, which occasionally is forgotten. The sum of the total under a probability distribution (continuous variable) or probability mass (discrete variable) must be 1. We can plot a distribution across <span class="math inline">\(x\)</span> that displays the probabilities across values of <span class="math inline">\(x\)</span>. For a discrete distribution, the <span class="math inline">\(y\)</span> axis represents the probability of observing discrete outcome <span class="math inline">\(x\)</span>; for a continuous outcome, the <span class="math inline">\(y\)</span> axis represents a ratio, which is the probability of observing <span class="math inline">\(x\)</span> within an (infinitesimally small) interval divided by the width of that interval. This “density” need not be 1 – in fact, it’s often not – rather, the area under that distribution must be 1. Kruschke (2011) uses a nice analogy. Consider a sponge. The mass of the sponge represents the probability. Density is <span class="math inline">\(mass/volume\)</span>. If we squeeze the sponge, the mass doesn’t change, though it’s density clearly does. Here, volume, is simply the range of <span class="math inline">\(x\)</span>.</p>
<p>We should also be clear about notation. Oftentimes, we deal with two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We’ll often refer to things such as “What is the probability of observing <span class="math inline">\(x\)</span> averaged, or marginalized, across <span class="math inline">\(y\)</span>?”</p>
<p>This is relatively easy to envision in a 2x2 table, in which we sum across rows or columns. The appropriate operation is then:</p>
<p><span class="math display">\[p(x)=\sum_y p(x_i, y_i) \hspace{0.5in}  \texttt{Marginal Probability}\]</span></p>
<p>Note, this only applies to a categorical distribution. Though, as we’ll see, the continuous version is really just an extension of this.</p>
<p>Distributions are also used to describe continuous variables. In fact, much of the applications in this class will assume a continous distribution, even if we only observe a discrete response option. In your first semester statistics course, you reviewed the properties of a variety of continuous distributions – I assume, the <strong>normal</strong>, <strong>poisson</strong>, <strong>student’s t</strong>, <strong>F</strong>, and so forth. One can use univariate calculus – integration, in particular – to calculate various areas under the curve. If this is foreign to you, because you forgot it or didn’t quite grasp the concept from your first semester class, I strongly recommend you go back and review this material. We cannot spend much time on reviewing the basics of integration, and we will use it frequently in this class. Back to the issue at hand. For a continuous bivariate density, then</p>
<p><span class="math display">\[p(x)=\int p(x, y) dy \hspace{0.5in}  \texttt{Marginal Probability}\]</span></p>
<p>Notice here the only difference is the <span class="math inline">\(\int\)</span> instead of <span class="math inline">\(\sum\)</span>. But, it’s useful to think of them as calling for the same thing. Then the <span class="math inline">\(dy\)</span> simply means “averaged across y.” Sometimes we say that this operation entails <em>integrating out y</em>, or <em>averaging across y.</em></p>
<p>There are also conditional probabilities; these are not the same as marginal probabilities. For instance, we might ask, ``what is the probability that a wrestler has died, given he/she is 35-40?’’ So, <span class="math inline">\(p(X=(death) | Y=35-40)\)</span>. The conditional probability is always,</p>
<p><span class="math display">\[p(x|y)=p(x,y)/p(y)\hspace{0.5in}  \texttt{Conditional Probability}\]</span></p>
<p>where, <span class="math inline">\(p(x)=\sum_y p(x, y)\)</span> or <span class="math inline">\(p(x)=\int p(x, y) dy\)</span>. In other words, we are taking the joint probability of two things happening, here <span class="math inline">\(p(x,y)\)</span>, and dividing the joint probability by the marginal probability of observing y, <span class="math inline">\(p(y)\)</span>. Think of it as the joint probability weighted by the marginal probability.</p>
<p>And, finally, we will almost always assume independence, such that the probability of one outcome does not depend on the probability of a second outcome. In a bivariate distribution, this means</p>
<p><span class="math display">\[p(x,y)=p(x) p(y) \hspace{0.5in}  \texttt{Independence}\]</span></p>
<p>This is actually quite intuitive if you think about it. If <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(p(y)\)</span> are entirely unrelated – knowing one does not help you know the other – then the probability of observing the two events is simply the product. If I flip a coin from 1989 and one from 1977, the flips are independent. Knowing the outcome of the 1989 coin flip is inconsequential for the outcome of the 1977 coin flip. By extension, if two events are independent, then the conditional probability <span class="math inline">\(p(x|y)=p(x)\)</span>. Think briefly about what this means in context of the coin flip. Given that the Canadian coins is head, what is the probability that the 1989 coin is heads?</p>
<p>We can extend these ideas further, leveraging these three basic principles to what is known as <strong>Bayes’ Rule</strong>, named after the mathematician Reverand Thomas Bayes.</p>
</div>
<div id="conditional-probabilities-continued-bayes-rule" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Conditional Probabilities, Continued: Bayes’ Rule<a href="introduction.html#conditional-probabilities-continued-bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the probability of observing <span class="math inline">\(x\)</span> given <span class="math inline">\(y\)</span>. This probability can be expressed as the joint probability of observing <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> divided by the marginal probability of observing <span class="math inline">\(y\)</span>. That is,</p>
<p><span class="math display">\[p(x|y)=p(x,y)/p(y)\]</span></p>
<p>We can just rearrange things to find <span class="math inline">\(p(y|x)\)</span>. First multiply the equation by <span class="math inline">\(p(y)\)</span></p>
<p><span class="math display">\[p(x|y) p(y)=p(x,y) \]</span></p>
<p>But, remember, we can find the joint probability, <span class="math inline">\(p(x,y)\)</span> as</p>
<p><span class="math display">\[p(y|x) p(x)=p(y,x) \]</span></p>
<p>Meaning,</p>
<p><span class="math display">\[p(x|y) p(y)=p(y|x) p(x)\]</span></p>
<p>Bayes’ Rule is simply,</p>
<p><span class="math display">\[p(y|x) =[p(x|y) p(y)]/p(x)\]</span></p>
<p>Notice, that we could simply solve this by inverting the rows and columns in the above example.</p>
<p>We’ll rely heavily on this basic principle, extending it in a number of interesting ways. For now, it’s more than sufficient to simply understand that Bayes’ Rule is really just a reordering of what we know about conditional probabilities.</p>
<p><strong>Why p(y|x) is important – but misunderstood</strong></p>
<p>The <strong>Monty Hall Experiment</strong> comes from the classic game show, “Let’s Make a Deal.” Here’s how it goes. There are three doors. Behind one door is a substantial prize – a car – behind the others are goats (undesirable prizes).</p>
<p>The contestant chooses one of three doors, but doesn’t open it. The host then chooses a door – obviously not the one with a car – opens it, revealing a goat. The contestant offers the option of staying with their original choice, or switching doors.</p>
<p><em>Should the contestant stick with the original choice or switch to the unopened door?</em></p>
<p>To begin – and knowing nothing else –</p>
<p><span class="math display">\[ P(Car, A) = P(Car, B) = P(Car, C)=1/3\]</span> Let’s just assume you pick <em>Door A.</em> This makes sense – why have a preference for a particular door, if each is equally likely to contain the car? So, Monte Hall then opens <strong>Door B</strong>, revealing a gaot.</p>
<p>Firstly, you want to know the probability that Monty opened door B given the car is behind door A. Since you chose door A, Monty could have opened either door B or door C.</p>
<p>Let’s break down the problem. The probability that Monte Hall opens <em>Door B</em> given the car is behind <em>Door A</em> is 0.5. Remember, Monty is responding to your choice. You choose A, so he can open <em>either</em> B or C, and will always pick one with the goat.</p>
<p><span class="math display">\[ P(OpenB | Car, A) = 1/2\]</span></p>
<p>We know even more though. What is the probability that Monty Hall opened door B given the car is behind door B? It’s zero! Monty will never reveal the door with the car, as that would just mean giving away a car, absent any real contestant participation.</p>
<p><span class="math display">\[ P(OpenB | Car, B) = 0\]</span></p>
<p>The last probability to consider is the probability that Monty opened door B given the car is behind door C.</p>
<p><span class="math display">\[ P(OpenB | Car, C) = 1 \]</span></p>
<p>This is 1 for a simple reason. If you choose A, and he knows the car is behind C, he will always open B.</p>
<p>Prepared with this, let’s invert the problem.</p>
<p><span class="math display">\[p(Car,A|OpenB)?\]</span> <span class="math display">\[p(Car,B|OpenB)?\]</span>? <span class="math display">\[p(Car,C|OpenB)?\]</span></p>
<p><span class="math display">\[p(Car,A|OpenB) = {{1/3 * 1/2}\over{1/3*1/2+ 1/3*0 + 1/3*1}} = 1/3\]</span></p>
<p><span class="math display">\[p(Car,B|OpenB) = {{1/3 * 0}\over{1/3*1/2+ 1/3*0 + 1/3*1}} = 0\]</span></p>
<p><span class="math display">\[p(Car,C|OpenB) = {{1/3 * 1}\over{1/3*1/2+ 1/3*0 + 1/3*1}} = 2/3\]</span> Don’t worry if you got it wrong. I did – I still find myself inclined to mistrust the result, though it is correct. It was the focus of famous Parade Magazine article, it’s been discussed in the New York Times, statisticians have debated the question, and it was the topic of a <em>MythBusters</em> television episode. By far the most common result is to say it doesn’t matter if you switch doors, acknowledging that the car resides behind Door A and B is equal, 0.5. This of course, is incorrect, which we can see with the application of Bayes’ Theorem.</p>
</div>
<div id="two-traditions" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Two Traditions<a href="introduction.html#two-traditions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will rely on probability densities a lot in this class. There are a variety of ways to describe, or summarize a distribution of data (or a theoretical distribution). Much of this class focuses on probability. Chapters 2 and 3 in King (1998) are also good references.</p>
<p>In statistics, we often (indirectly) use probabilities in the relativistic sense. We think of it as trials or experiments that are repeatable, aka the frequentist interpretation. For instance, the estimate of a particular candidate winning the nomination may be 0.55. If we were able to conduct a primary election over and over and over again, in the long run, we are assuming the candidate wins 55 out of 100 times. On any given trial though, they’ll either win or lose, just as you’ll only win or lose if you play a game of chance.</p>
<p>It’s well established in the psychology literature that humans are not great at processing probabilities. Our attention is drawn to emotionally evocative events – evident in the so called “availability heuristic,” and humans don’t accurately process base rates, the “accessibility heuristic.” For instance, shortly after the September 11 attacks, many Americans chose to drive rather than fly, despite the fact that driving was still far more dangerous – in terms of fatalities – than flying. Thee probability of dying in a terrorist attack in the U.S. has always been far less than dying in an automobile accurate.</p>
<p>In the social sciences in particular, we use probability in a <strong>subjective</strong> sense, by referring to the probability as a statement of certainty. While not inherently inaccurate, certainty means something different in the frequentist tradition, where parameters are fixed and we sample from a population.</p>
<p>When we fit a model – perhaps by minimizing the sum of squared residuals – we are generating a statement about the ability of our data being produced by a particular model, <span class="math inline">\(p(Y|M)\)</span>. This is essentially what we do when we compare model fit, examining how the overall model changes in its predictive power. The problem is we may be inclined to make a statement about the model given the data. What is the probability, for instance, that a parameter falls in a particular range? But this requires a bit more information.</p>
<p><span class="math display">\[p(M|D) =[p(D|M) p(M)]/p(D)\]</span></p>
<p>The inverse probability is then a function of the likelihood, as well as the probability of observing the model the probability of observing the data. It turns out that since the denominator is a normalizing constant, which renders the numerator into a valid probability, it is proportional to the product absent <span class="math inline">\(p(D)\)</span>.</p>
<p><span class="math display">\[p(M|D) = p(D|M) p(M)/p(D)\]</span></p>
<p><span class="math display">\[p(M|D) \propto p(D|M) p(M)/p(D) \]</span></p>
<p>What this establishes is these two principles: Frequentism versus Bayesian inference. Typically, we’re interested in the Bayesian version of the probability statement – “what is the probability that our data produced a model?” The frequentist version, which draws on what is called the likelihood, really posits something different, and that is, “what is the probability that a model produced our data?”</p>
<p>At this point, it’s worthwhile to use an example. We’ll use the principle of <em>maximum likelihood</em> first, since the likelihood equation is part of Bayes’ Rule (do you see why?). Then, we’ll apply it to Bayesian statistics. We’ll also start really, really simple example, with a single parameter: The case of flipping a coin.</p>
<div id="an-example-bernoulli-trials" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> An Example: Bernoulli Trials<a href="introduction.html#an-example-bernoulli-trials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’re going to operate from the assumption that a coin is fair, which simply means the probability of observing an H is the same as the probability of observing T. In this case, <span class="math inline">\(y \in [0 , 1]\)</span>. And let’s just say that:</p>
<p><span class="math display">\[y = {1, H, \theta=0.5}\atop  {y=0, T, 1-\theta=0.5} \]</span></p>
<p>So, we’re just labeling heads 1, tails 0. Each has an equal probability. Let’s call this population parameter governing the behavior of the coin, <span class="math inline">\(\theta\)</span>. This isn’t a trivial assumption – it’s central to <em>frequentist</em> inference – the parameter is fixed. The assumption is it exists, and governs the behavior the coin. Because we know what <span class="math inline">\(\theta\)</span> is, by assuming the coin is fair, it is easy to find the probability of observing a particular string of heads, tails, or some combination.</p>
<p>Let’s also assume independent trials. What this means is one trial is independent of a subsequent trial. Then the joint probability of observing a (H,H) is simply <span class="math inline">\(0.5 \times 0.5=0.25\)</span>.But, perhaps we have some reason to expect the coin is not fair, and <span class="math inline">\(\theta=0.3\)</span> (the coin is biased in favor of T). Then, the probability of observing a H,H,T is <span class="math inline">\(0.3 \times 0.3 \times 0.7\)</span>}. Recall from the previous section we could simply express these independent coin flips in a formal expression, as</p>
<p><span class="math display">\[p(heads)=\theta^y(1-\theta)^{1-y}\]</span></p>
<p>And, across <span class="math inline">\(n\)</span> independent trials trials,</p>
<p><span class="math display">\[p(k)=\prod\theta^y_i(1-\theta)^{1-y_i}\]</span> <span class="math display">\[=\theta^k(1-\theta)^{n-k}\]</span></p>
<p>Since <span class="math inline">\(\textbf{y}=y_1, y_2...y_n\)</span> is observed and we are making an assumption about the constituent probabilties, all we need to do is multiply the probability of each outcome, here denoted by <span class="math inline">\(\prod\)</span></p>
<p>Likewise, we could just use the binomial distribution to calculate this probability (call <span class="math inline">\(K\)</span> an observed sequence).</p>
<p>$$p(K | \theta, N)= {n \choose k} \theta^K(1-\theta)^{N-K}$$</p>
<p>Turn the question on it’s head (sorry). We’ve assumed that <span class="math inline">\(\theta\)</span> is known, allowing us to generate a valid probability distribution for some observed sequence. However, <span class="math inline">\(\theta\)</span> – the statistical parameter – is usual what we want to estimate. It exists in the population, but it is not directly accessible. However, we assume it could accessed, if we had resources to access the total population – like a <em>census</em>. Of course, we rarely can access the whole population – if we could, what would be the point of inference – and instead we make an inference about this parameter. It’s perhaps easiest to consider with coin flips. For a given coin, we assume <span class="math inline">\(\theta\)</span>. If we approach the trials with an assumption of a fair coin, we simply posit that <span class="math inline">\(\theta=0.5\)</span>. But, what if we are not able to generate such a concrete assumption about <span class="math inline">\(\theta\)</span>?</p>
<p>Instead, suppose we only have access to an observed series coin flips. Instead of approaching the problem from the issue of, <em>What is the probability of observing two heads in a series of 10 flips, given a fair coin?</em> we might ask, “Given 2 observed heads out of 10, what is the most probable value of <span class="math inline">\(\theta\)</span>”? Or, “Is the coin fair?” It shouldn’t take much convincing that this is a qualitatively different question. It’s also the question we commonly ask ourselves in applied research. In particular, <strong>Given the data available, what is the most likely parameter or set of parameters to generate the data?</strong> In this case, “parameter” may be an estimate about how many people will vote for a candidate, a slope coefficient, and so forth. An alternate way to think of this is <span class="math inline">\(p(D|M)\)</span> – or what is the probability of observing the data, given the model (King 1998).</p>
<p>Although you may not have seen this notation yet, all our statistic models assume we want to maximize the likelihood that an estimated population value produced a dataset, called <span class="math inline">\(D\)</span>, so <span class="math inline">\(p(D | \theta)\)</span>. This is also why I’ve been following this convention. Consider the logic of minimizing the sum of squared errors. If <span class="math inline">\(\theta\)</span> simply represent a vector of slope coefficients, recall that the logic of OLS is to minimize the squared discrepancy between the observed and predicted values. Though we can estimate an infinite number of <span class="math inline">\(\theta\)</span> values, only one will meet the criterion of minimizing the sum of squared residuals. This is equivalent to asking ourselves, ``what is a set of <span class="math inline">\(\theta\)</span> values that maximizes the likelihood of observing a particular dataset?’’ in that finding <span class="math inline">\(\theta\)</span> that minimizes the sum-of-squared residuals will also maximize the probability of observing a particular dataset.</p>
<p>This is the logic underlying a technique that we will use throughout this semester, which is called “maximum likelihood.” In the linear model, the maximum likelihood estimator and the OLS estimator will yield the same results; yet, the logic of minimizing SSR is not applicable to many other data situations (e.g., a binary dependent variable). Thus, we may use the logic of ML to estimate a variety of models.</p>
<p>Returning to the motivating example: What is <span class="math inline">\(\theta\)</span> in a series of coin flips. Maximum likelihood is a technique to estimate parameters in a model, not unlike the principle of least squares. The logic – and not so much math – is as follows. Let’s call <span class="math inline">\(\theta\)</span> some set of parameter estimates and <span class="math inline">\(D=(y_1....y_n)^T\)</span> is the observed data. The probability of observing vector, <span class="math inline">\(D|\theta\)</span> is simply the product of all individual values of <span class="math inline">\(y_i | \theta\)</span>, if values are independently observed. Thus, the probability of observing H,H,T with a fair coin is simply <span class="math inline">\(0.5 \times 0.5 \times 0.5\)</span>.</p>
</div>
</div>
<div id="simulation-and-the-likelihood-function" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Simulation and the Likelihood Function<a href="introduction.html#simulation-and-the-likelihood-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Assume we observe 7 heads, 3 tails. What’s our best guess of theta?</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="introduction.html#cb4-1" tabindex="-1"></a>theta<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb4-2"><a href="introduction.html#cb4-2" tabindex="-1"></a><span class="fu">paste</span>(<span class="fu">paste</span>(<span class="st">&quot;Possible value:&quot;</span>, <span class="fu">head</span>(theta)), <span class="st">&quot;...&quot;</span> )</span></code></pre></div>
<pre><code>## [1] &quot;Possible value: 0 ...&quot;    &quot;Possible value: 0.01 ...&quot;
## [3] &quot;Possible value: 0.02 ...&quot; &quot;Possible value: 0.03 ...&quot;
## [5] &quot;Possible value: 0.04 ...&quot; &quot;Possible value: 0.05 ...&quot;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="introduction.html#cb6-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Assume we observe 7 heads, 3 tails. What&#39;s our best guess?</span><span class="sc">\n</span><span class="st"> </span><span class="sc">\n</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Assume we observe 7 heads, 3 tails. What&#39;s our best guess?
##  
## </code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="introduction.html#cb8-1" tabindex="-1"></a><span class="fu">plot</span>(theta, theta<span class="sc">^</span><span class="dv">7</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="introduction.html#cb9-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The maximum value of the distribution is:&quot;</span> ,theta[<span class="fu">which.max</span>(theta<span class="sc">^</span><span class="dv">7</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">3</span>)])</span></code></pre></div>
<pre><code>## The maximum value of the distribution is: 0.7</code></pre>
<p>Instead of having a known probability, we can work the other direction and calculate the most likely value for <span class="math inline">\(\theta\)</span> given an observed data set. To keep things tractable, assume 10 flips, and we observe 7 heads. That is, <span class="math inline">\(L(\theta | \sum y_i=7, N=10)\)</span> – we may generate a value for <span class="math inline">\(\theta\)</span> that maximizes the probability of observing 7/10 heads.</p>
<p>Formally,
<span class="math display">\[L(\theta)=\Pi p(y_i | \theta)\]</span>.</p>
<p>Thus, the likelihood equation is the probability of observing <span class="math inline">\(y\)</span> given some <strong>best guess</strong> of <span class="math inline">\(\theta\)</span>. Below, we’ll very briefly develop a few techniques to formulate this guess</p>
<p>We’ve now observed the data, and instead of knowing <span class="math inline">\(\theta\)</span>, we can estimate the most plausible value of <span class="math inline">\(\theta\)</span>, considering our data. If we flip a coin ten times and observe seven heads, what is a value of <span class="math inline">\(\theta\)</span> that is most likely to produce this sequence of results? It’s 0.7; the maximum likelihood estimate of <span class="math inline">\(\theta\)</span> is 0.7.</p>
<p>All the code does is take the probability of observing a head at each trial and multiplied these together. If we simulate values of <span class="math inline">\(\theta\)</span>, we find a single peaked function with a maximum value of 0.7.</p>
<p>In particular, we have identified a value of <span class="math inline">\(\theta\)</span> in the population that was most likely to produce the observed data. In other words, we assume:</p>
<p><span class="math display">\[p(y|\theta)=\prod_{n=1}^Np(y_n|\theta)=\prod_{n=1}^N\theta^{y_n}(1-\theta)^{1-{y_n}}\]</span>
(Bishop 2006, page 69).</p>
<p>If the sequence of results is <span class="math inline">\({H,H, T, H, T, T, T, T, T, T}\)</span>, if <span class="math inline">\(\theta=0.1\)</span>, then we multiply <span class="math inline">\(0.1 \times 0.1 \times 0.9 \times 0.1 \times 0.9^6\)</span>. By doing this for every value of <span class="math inline">\(\theta\)</span>, we want to find the highest probabilty associated with <span class="math inline">\(\theta\)</span>. This is precisely the logic of maximum likelihood: Observe a dataset and find a value of <span class="math inline">\(\theta\)</span> that is most likely to have produced that dataset. What you can see is that the function is peaked. There is only one value that maximizes the ``likelihood function’’ which is simply:</p>
<p><span class="math display">\[\begin{eqnarray}
p(y|\theta)=\prod_{n=1}^Np(y_n|\theta)=\prod_{n=1}^N\theta^{y_n}(1-\theta)^{1-{y_n}}
\end{eqnarray}\]</span></p>
<p>I’ve solved the problem with a simulation. In fact, that’s not required. In this case, there is a closed form solution to the problem. In particular, we take the logarithm of the likelihood function, and solve by taking partial derivatives and setting these values to zero.</p>
<p>In this example, you’ve probably noticed that the maximum likelihood estimate for <span class="math inline">\(\theta\)</span>, given <span class="math inline">\(n\)</span> Bernoulli trials is simply <span class="math inline">\(k/n\)</span>, where <span class="math inline">\(k=\sum y_i\)</span>. We’ll rely on this logic throughout the semester – and we’ll extend this considerably – but for now it’s really just important to conceptually understand the motivation, which is to find the most likely value of <span class="math inline">\(\theta\)</span> that produced the observed distribution of data.</p>
<p>As King (1998) notes, “<strong>Maximum Likelihood Estimation</strong> is a theory of point estimation that derives in this very direct way from the likelihood function. The maximum is not always a very good summary of the entire likelihood function, but it is very convenient and often useful” (p. 24)</p>
<div id="bayesian-analysis" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Bayesian Analysis<a href="introduction.html#bayesian-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This likely seemed like a really roundabout way to approach the obvious estimate of <span class="math inline">\(\theta\)</span>. Yet, often the likelihood equation (in this case the binomial) is not so simple. We’re also left with a less than intuitive probability statement – what is the value of <span class="math inline">\(\theta\)</span> that maximized the probability of observing a set of observed heads? We’re not really able to make a probablistic statement about <span class="math inline">\(\theta\)</span> from the method. For instance, we cannot something like, “based on the data, I am 90% certain that its value lies between 0.6 and 0.7, which corresponds to the conclusion that the coin is unfair”!</p>
<p>But, we can invert the probability statement and model <span class="math inline">\(p(\theta|Y)\)</span> by applying Bayes’ rule.There are many good ways to acquaint onself with Bayesian analysis. By far the best (in my opinion), is the third edition Bayesian Data Analysis, which is a fantastic introduction to Bayesian analysis for the social sciences, written by Andrew Gelman and colleagues (Gelman, Carlin, Stern, Dunson, Vehtari and Rubin 2014). Unlike some of the alternatives, it presents a relatively useful balance of technical details and more practical considerations.} In fact, Thomas Bayes (an English reverend and mathematician) and independently, Pierre LaPlace, thought about the problem somewhat differently, by focusing on <span class="math inline">\(\theta\)</span> (Gelman, Carlin, Stern, Dunson, Vehtari and Rubin 2014).</p>
<p>We can’t really make a probabilistic statement about <span class="math inline">\(\theta\)</span>; we can only find a value that is most likely to have produced a dataset. This is functionally equivalent to what you’ve learned about inference thus far. Probabilities don’t allow us to make probabilistic statement about a parameter, rather, they pertain to a procedure. If I say, <strong>The 95% confidence interval around my estimates of a candidate’s share of the New Hampshire Primary vote is [32, 36], what does that mean?</strong> Can I say, there is a 95% chance that the true parameter is between 32 and 36? We could use Bayes’ Rule to flip this logic on it’s head. Instead of a confidence interval, let’s just use <span class="math inline">\(pr(TrumpWins)=\theta\)</span>}</p>
<p>Let’s just assume the following for <span class="math inline">\(Y={H, T, H, H, T}\)</span>. What is the $pr(| Y $. We already know the maximum likelihood estimator of <span class="math inline">\(\theta=0.60\)</span> (why?). But recall, we also need to make a statement about <span class="math inline">\(p(Y)\)</span> as well as <span class="math inline">\(p(\theta)\)</span>. We have a probability density for <span class="math inline">\(Y \sim binomial(Y, N, P)\)</span>. But, this density is multiplied by our ``prior beliefs’’ about <span class="math inline">\(\theta\)</span>. There isn’t sufficient space here to outline all the steps, but suffice it to say that because we must multiply two probability densities, it’s more tractable if they are conjugate distributions. This simply means the posterior, the end result, is the drawn from the same family of PDFs as the prior.</p>
</div>
</div>
<div id="quick-review" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Quick Review<a href="introduction.html#quick-review" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Correlated random variables</strong>. If we have two independent random variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, with means <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_y\)</span> and standard deviations, <span class="math inline">\(\sigma_x\)</span> and <span class="math inline">\(\sigma_y\)</span>, then <span class="math inline">\(x+y\)</span> yields <span class="math inline">\(\mu_x+\mu_y\)</span>. The standard deviation then is <span class="math inline">\(\sqrt{\sigma^2_x+\sigma^2_y+\rho\sigma_x\sigma_x}\)</span>, where <span class="math inline">\(\rho\)</span> is the correlation between the variables (Gelman and Hill 2009, p.14). We often assume the two variables follow a multivariate normal distribution, <span class="math inline">\(z_k \sim N(\mu_k, \Sigma_{kk})\)</span>. <span class="math inline">\(\mu_k\)</span> represents the means of the random variables, <span class="math inline">\(\Sigma\)</span> represents a covariance matrix, with variances on the diagonal and covariances on the off diagonal.</p>
<p><strong>Variable Transformations</strong>. Often, we transform variables (perhaps to restore normalcy). A common technique is to transform a variable by taking its natural logarithm. The exponential of the mean of these log values is called the geometric mean; the geometric standard deviation is the exponential of the standard deviation of logarithmic values. These are the means and standard deviations on the original scale, which are calculated as <span class="math inline">\(exp(\mu+0.5 \sigma^2)\)</span> and <span class="math inline">\(exp(\mu+0.5 \sigma^2)\sqrt{exp(\sigma^2)-1}\)</span>, respectively (Gelman and Hill, p.15).</p>
<p>Moreover, if we have two independent variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, with means <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_y\)</span> and standard deviations, <span class="math inline">\(\sigma_x\)</span> and <span class="math inline">\(\sigma_y\)</span>, then <span class="math inline">\(x+y\)</span> yields <span class="math inline">\(\mu_{x+y}\mu_x+\mu_y\)</span>. The standard deviation then is <span class="math inline">\(\sqrt{\sigma^2_x+\sigma^2_y+\rho\sigma_x\sigma_x}\)</span> (Gelman and Hill 2009, p.14). We often assume the two variables follow a multivariate normal distribution, <span class="math inline">\(z_k \sim N(\mu_k, \Sigma_{kk})\)</span>. <span class="math inline">\(\mu\)</span> represents the means of the random variables, <span class="math inline">\(\Sigma\)</span> represents a covariance matrix, with variances on the diagonal and covariances on the off diagonal.</p>
<p>Recall the <strong>central limit theorem</strong> states that by drawing repeated samples and calculating the means, then the distribution of sample means will be approximately normal. A , which we commonly work with, is a model applied to a sample from a population; we in turn use that model to make an inference about a population. In fact, a test-statistic or estimand is used in this model to draw an inference about a <strong>population parameter</strong>. Statistics are often called parameter estimates.</p>
<p>The <strong>standard error</strong> of a statistic represents uncertainty about the parameter estimate. In the case of a mean, recall that the central limit theorem allows us to estimate the standard deviation of sample means, <span class="math inline">\(\sigma/\sqrt{n}\)</span>. When dealing with proportions, the standard error is <span class="math inline">\(\sqrt{p(1-p)/n}\)</span>. Say we are interested in the difference between two proportions, we must then calculate the standard deviation of the differences, or <span class="math inline">\(\sqrt{sd_{p1}^2+sd_{p2}^2}\)</span></p>
<p><strong>The Expected Value</strong>. The expected value is the ``average’’ value over many draws – it’s useful to think of it from the perspective of frequentism. For a discrete distribution, then:</p>
<p><span class="math display">\[E(x_i)=\sum_{i}^{K} x_i p(x_i)\]</span></p>
<p>Again, this doesn’t tell us anything about a single, or even predicted, <span class="math inline">\(x_i\)</span> value. It is the value of <span class="math inline">\(x_i\)</span>, weighted by <span class="math inline">\(p(x_i)\)</span>. Take a simple example, flipping a coin. Let’s say <span class="math inline">\(H=Y=1\)</span>, <span class="math inline">\(T=Y=0\)</span>. As such, if we flip a coin 10 times, then <span class="math inline">\(E(Y_i)=\sum y_i  f(y)=(0.5)^{10}\)</span>.</p>
<p>We have conceptually the same thing for a continuous variable, where</p>
<p><span class="math display">\[E(x)=\int_{-\infty}^{\infty}x f(x)dx\]</span>.</p>
<p>Again, it’s simply the sum of the occurence weighted by the probability of that occurence. If <span class="math inline">\(y=f(x)=a+bx\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are simply constants, then <span class="math inline">\(E(y)=E(a+bx)=aEx+b\)</span>. The expected value of a fixed value or constant value is a constant. You should recognize this from the linear regression course, <span class="math inline">\(E(Y)=a+b\bar{X}\)</span>.</p>
<p>The expected value is also called the <em>first moment</em> of a distribution.</p>
</div>
<div id="properties-of-estimators" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Properties of Estimators<a href="introduction.html#properties-of-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’re often concerned with making inferences about a population from a sample. Recall, in the frequentist tradition, we think of parameters as fixed characteristics of the population; statistics are derived from sample(s) drawn from the population. They are often qualified with some degree of uncertainty. An estimator is the formula or equation used to represent what we think is the process governing a parameter. So, <span class="math inline">\(y_i=\alpha+\beta x + \epsilon\)</span> forms the relationship between x and y in the population, governed by fixed parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and an error term. This is not the estimator. The estimator is the method we then use to guess these parameters. Ordinary least squares is an estimator. It’s one of many estimators. Much of POL 681 was oriented around showing it is the best linear unbiased estimator, in particular conditions.</p>
<p>The issue is that we only have a sample, or set of samples. Thus we wish to draw an inference about the population from the sample. The various properties of an estimator can be explored in several ways. One is to assume a sample is observed over repeated trials, so we have repeated samples. We could then perform our estimation procedure across samples and compile these estimates. Recall the logic of the central limit theorem, for instance.</p>
<p>If we were able to repeatedly draw samples and run our estimation procedure, we are left with a . We can then explore properties of an estimator by examining this distribution. Most notably, the distribution will have mean</p>
<p><span class="math display">\[E(\hat {\theta})\]</span></p>
<p><span class="math display">\[var(\hat{\theta})=E(\hat{\theta}-E(\hat{\theta}))^2\]</span></p>
<p>With <span class="math inline">\(\sqrt{var(\hat{\theta})}\)</span> as the standard deviation of the sampling distribution. We can then establish three statistics to explore the properties of an estimator. The is the deviation between our estimator <span class="math inline">\(\hat{\theta}\)</span> and the population parameter (<span class="math inline">\(\theta\)</span>). There will always be some degree of error by drawing a sample from a population. We may wish to minimize this sampling error, but we shouldn’t expect it to be zero.</p>
<p><strong>Bias</strong> is the difference between the expected value of an esimator relative to the true population parameter, i.e., <span class="math inline">\(\hat{\theta}-\theta\)</span>. Note how this is different from the sampling error. The sampling error refers to the difference for a single estimate produced from our estimator. Bias is the average of many samples analyzed with an estimator.</p>
<p>We’re often also interested in how much an estimator varies about a true population parameter. Here, let’s define the <strong>Mean Squared Error</strong> (MSE), or <span class="math inline">\(E(\hat{\theta}-\theta)^2\)</span>. Again, notice the difference here. Here, we are concerned with how much an estimator varies around the true population parameter. It can be shown that the MSE may be rewritten as</p>
<p><span class="math display">\[E(\hat{\theta}-E(\hat{\theta}))^2 +[E(\hat{\theta}-\theta)]^2\]</span></p>
<p>In other words, the MSE is a function of the variance of the estimator and the bias of the estimator. We generally wish for this value to be small. For instance, if we were to compare two estimators, we should prefer the estimator that is unbiased with minimum variance, which of course translates to having the smallest MSE.</p>
</div>
<div id="finite-sample-properties" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Finite Sample Properties<a href="introduction.html#finite-sample-properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Much of what you’ve learned in POL 681 pertained to OLS and <strong>small sample properties</strong>. Some estimators have desirable properties even when a sample is small. For instance, OLS has desirable small sample properties. When we refer to small sample properties, this typically entails.</p>
<p>Unbiasedness, or <span class="math inline">\(E(\hat{\theta})=\theta\)</span>. Over repeated samples, the mean of the sampling distribution will equal the true population parameter. Although it may seem that unbiaseness is sufficient to draw conclusions about an estimator, it’s not. The reason is that it refers to repeated samples. We have no idea if a parameter estimate from a single sample is near to the true population value.</p>
<p>Efficiency, or</p>
<p><span class="math display">\[var(\hat{\theta})&lt;var(\tilde{\theta})\]</span>.</p>
<p>Compared to other estimators, our estimator should have smaller variance.</p>
<p>In some cases, there is a tradeoff, in that we may have an estimator that is known to be biased, but it has smaller variance. Or, we have an estimator that is unbiased, but has huge variance. In fact, we can operationalize this further by simply using the MSE. We may prefer an estimator that has the smallest MSE, even if it’s known to be biased.</p>
<p>An example of this is ridge regression. Recall, that in ridge regression, in some circumstances, we’ll have an estimator with bias, but smaller variance – it will have a lower MSE.</p>
<p>Something that we haven’t sufficiently established yet is the large or asymptotic properties of an estimator. What happens to the behavior of the sampling distribution as the sample size approaches infinity.</p>
</div>
<div id="asymptotic-properties" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Asymptotic Properties<a href="introduction.html#asymptotic-properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>These refer to the properties of an estimator as the sample size approaches infinity. In other words, as the sample size gets larger and larger, what happens to the characteristics of an estimator? An estimator is said to be <em>asymptotically unbiased</em> if</p>
<p><span class="math display">\[\lim_{n\to\infty} E(\hat{\theta})=\theta\]</span></p>
<p>A perfect example of this is the estimate of the sample variance; you may recall that the population variance is:</p>
<p><span class="math display">\[\sigma^2={{1}\over{n}}(x-\mu_{x})^2\]</span></p>
<p>But the sample variance is:</p>
<p><span class="math display">\[\sigma^2={{1}\over{n-1}}(x-\bar{x})^2\]</span></p>
<p>But, the population variance is an unbiased estimator as the sample size increases. In particular,</p>
<p><span class="math display">\[\lim_{n\to\infty} E(\hat{\sigma^2})=\lim_{n\to\infty} [{{n-1}\over{n}}]\sigma^2=\sigma^2\]</span></p>
<p>Note that as n approaches infinity, the term <span class="math inline">\([{{n-1}\over{n}}]\)</span> approaches 1 and the estimator converges to the population parameter.</p>
<p>An estimator is <em>consistent</em> if:</p>
<p><span class="math display">\[P\lim_{n\to\infty}(|\hat{\theta}-\theta|&lt;d)=1\]</span></p>
<p>then,</p>
<p><span class="math display">\[Plim_{n\to\infty}(\hat{\theta})=\theta\]</span></p>
<p>Really, this is subtly different from asymptotic unbiasedness. What it means is that the probability that the estimator is not different from the population parameter is 1 as the sample size approaches infinity. Put slightly different, the probability increases to 1 that as the sample size increases the estimator converges to the population parameter. It is possible – in fact quite common – for an estimator to be biased but consistent. We sometimes refer to this as the bias/variance tradeoff. A prime example is the variance estimate above. When we divide by <span class="math inline">\(n\)</span> this is also called the MLE estimatate of the variance. It is biased in small samples, but consistent and asymptotically unbiased. I like to think of consistency more in terms of the variance of an estimator, such that <span class="math inline">\(P\lim_{n\to\infty}\)</span> represents the point at which the variance is 0 and the entire distribution collapses to one value – i.e., <span class="math inline">\(\lim_{n\to\infty} MSE(\hat{\theta}=0)\)</span></p>
<div id="the-bias-variance-tradeoff" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> The Bias Variance Tradeoff<a href="introduction.html#the-bias-variance-tradeoff" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By now, you should be intimately familiar with a traditional linear equation.</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1 x_{1,i}+\beta_2 x_{2,i}+\dots+\beta_k x_{k,i}+e_i\]</span></p>
<p>We can find these parameters by minimizing the residual sum of squares.</p>
<p><span class="math display">\[RSS=\sum_{i=1}^N=Y_i-(\beta_0+\beta_1 x_{1,i}+\beta_2 x_{2,i}+\dots+\beta_k x_{k,i})\]</span>
A problem that often arises in practice is that of collinearity – with finite samples, it is not uncommon to find that some variables are (imperfect) linear combinations of other variables, which doesn’t necessarily affect the parameter estimates, but will grossly exaggerate the standard errors. What does this mean? We end up with an unbiased, but inefficient estimate. This is rarely ideal. First, we cannot effectively test hypotheses and second unless the model is strongly informed by theory, it is possible that we could drop a variable. Of course, we know that if we drop a variable that should be in the model, then we risk biased estimates. This is one issue that is very common in applied statistics; theory may not be terribly informative and it is unclear whether a variable should be included in the model. Plus, if we include irrelelevent variables, this will promote “overfitting” whereby our “out-of-sample” predictions (think election forecasting) will be off.</p>
<p>We’ll encounter the so-called bias-variance tradeoff frequently in the class. Some estimators may be unbiased, but inefficient. This is a very real problem. Though unbiased sounds good, recall what it means: The expected value of the parameter estimate is the true population value. We approached it with relatively simple proofs – recall, the Gauss-Markov theorem – though we can also really easily see it in practice. Consider the following:</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="syllabus.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
